---
title: "p8105_hw5_anw2158"
author: "Aung Nay Win"
output: github_document
---

```{r, include=FALSE}
library(dplyr)
library(purrr)
library(ggplot2)
library(broom)
library(readr)
library(tidyr)
library(stringr)
```

### Problem 1
## Description of The Raw Data

uid: A unique identifier for each homicide incident.
reported_date: The date on which the homicide was reported, in a YYYYMMDD format.
victim_last: The last name of the homicide victim.
victim_first: The first name of the homicide victim.
victim_race: The race of the victim.
victim_age: The age of the victim.
victim_sex: The sex of the victim, indicated as Male or Female.
city: The city where the homicide occurred.
state: The state where the homicide occurred.
lat: The latitude coordinate of the homicide location.
lon: The longitude coordinate of the homicide location.
disposition: The status of the case

```{r}
homicide_data = read_csv("./data/homicide-data.csv") |> 
  mutate(city_state = paste(city, state, sep = ", ")) |> 
  group_by(city_state) |> 
  summarise(total_homicides = n(),
            unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))
```

Proportion Test for Baltimore, MD
```{r}
baltimore_data = homicide_data |>  filter(city_state == "Baltimore, MD")

prop_test_baltimore = 
  prop.test(baltimore_data$unsolved_homicides, baltimore_data$total_homicides)
tidy_baltimore = tidy(prop_test_baltimore)
```

Apply prop.test to Each City

```{r, warning=FALSE}
perform_tidy_prop_test = function(unsolved, total) {
  prop_test_result =  prop.test(unsolved, total)
  tidy(prop_test_result)
}

city_test_results = homicide_data |> 
  mutate(prop_test = pmap(list(unsolved_homicides, total_homicides), ~perform_tidy_prop_test(..1, ..2))) |> 
  unnest(prop_test)
```

```{r}
ggplot(city_test_results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  coord_flip() +
  labs(x = "City", y = "Proportion of Unsolved Homicides") +
  theme_minimal()
```

### Problem 2

```{r}
directory_path = "./data/problem2_data"
file_names = list.files(path = directory_path, pattern = "*.csv", full.names = TRUE)
```

```{r, message=FALSE}
process_file = function(file_name) {
  data = read_csv(file_name)
  
  subject_id = str_extract(file_name, "(?<=con_|exp_)[0-9]+")
  arm = ifelse(str_detect(file_name, "con"), "control", "experimental")
  
  data = data %>%
    mutate(subject_id = subject_id,
           arm = arm)
  
  return(data)
}


participant_data = map_df(file_names, process_file)
```

```{r}
tidy_participant_data = participant_data |> 
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    values_to = "observation",
    names_prefix = "week_"
  )
```

```{r}
ggplot(tidy_participant_data, aes(x = week, y = observation, group = subject_id, color = arm)) +
  geom_line() +
  labs(title = "Spaghetti Plot of Observations Over Time",
       x = "Week",
       y = "Observation",
       color = "Group") +
  theme_minimal()
```

Comment on differences between groups

- There is noticeable variability within each group, suggesting that individual responses to the conditions are not uniform. This could be due to individual differences in how participants react to the control and experimental conditions.

- In certain segments of the plot, there seems to be a divergence between the two groups, with the experimental group possibly showing a different pattern of observations compared to the control group. This could indicate an effect of the experimental intervention.

- The beginning and end points of the lines for each group might provide insights into the overall impact of the experiment. If the experimental group consistently ends at a different point than the control group, this could suggest a longitudinal effect of the treatment.

### Problem 3

```{r}
simulate_t_test = function(mu, sigma, n) {
  sample = rnorm(n, mean = mu, sd = sigma)
  test_result = tidy(t.test(sample, mu = 0))
  return(list(estimate = test_result$estimate, p.value = test_result$p.value))
}
```

```{r}
simulate_power = function(mu, sigma, n, num_sim = 5000) {
  sims = replicate(num_sim, simulate_t_test(mu, sigma, n), simplify = FALSE)
  estimates = sapply(sims, function(x) x$estimate)
  p.values = sapply(sims, function(x) x$p.value)
  
  power = mean(p.values < 0.05)
  mean_estimate = mean(estimates)
  mean_estimate_rejected = mean(estimates[p.values < 0.05])
  
  return(list(power = power, mean_estimate = mean_estimate, mean_estimate_rejected = mean_estimate_rejected))
}
```

```{r}
n = 30
sigma = 5
mu_values = 0:6
results = setNames(lapply(mu_values, simulate_power, sigma = sigma, n = n), mu_values)
```

```{r}
power_df =
  data.frame(mu = names(results), power = sapply(results, function(x) x$power))
estimate_df =
  data.frame(mu = names(results), estimate = sapply(results, function(x) x$mean_estimate))
estimate_rejected_df =
  data.frame(mu = names(results), estimate = sapply(results, function(x) x$mean_estimate_rejected))
```

```{r}
ggplot(power_df, aes(x = as.numeric(mu), y = power)) +
  geom_line() +
  labs(title = "Power vs. True Mean", x = "True Mean (mu)", y = "Power")
```
As the true mean (effect size) increases from 0 to 6, the power of the test also increases. This is a typical relationship because larger effect sizes make it easier to detect a difference from the null hypothesis (in this case,mu=0).When the true mean is 0, which aligns with the null hypothesis, the power of the test is at its lowest. Ideally, if the null hypothesis is true, the power should be equal to the significance level, indicating the rate of Type I errors (false positives). However, due to sampling variability in simulations, this might be slightly higher or lower in practice. There is a steep increase in power as the true mean moves away from 0. The test becomes more likely to correctly reject the null hypothesis as the difference between the true mean and the null hypothesis value becomes greater.The power curve seems to plateau as the effect size gets larger, approaching a power of 1 (or 100%).

```{r}
ggplot(estimate_df, aes(x = as.numeric(mu), y = estimate)) +
  geom_line() +
  geom_line(data = estimate_rejected_df, aes(y = estimate), color = "red") +
  labs(title = "Average Estimate of Mu vs. True Mean", x = "True Mean (mu)", y = "Average Estimate (mu_hat)")
```
No, the sample average of mu hat across tests for which the null is rejected is not equal to the true value of mu. This is because when the null is rejected, it often includes samples where the observed effect is larger due to sampling variability, leading to an overestimation of the true effect size. This phenomenon is more pronounced at smaller true effect sizes where the test power is lower.
